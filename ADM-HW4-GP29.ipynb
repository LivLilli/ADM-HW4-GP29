{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from scipy.spatial import distance_matrix\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bonus Clustering\n",
    "\n",
    "Implement the K-means algorithm (not ++: random initialisation) from scratch (without the library you used before). Are the clusters that you get similar to the ones you previously obtained?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "<img src=\"https://i2.wp.com/www.aprendemachinelearning.com/wp-content/uploads/2018/03/kmeans-3d-clusters.png\"/> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans:\n",
    "    def recomputeCetroid(self):\n",
    "        self.centroids = [np.array(self.clusters[item]).mean(axis = 0)  for item in self.clusters.keys()]\n",
    "\n",
    "\n",
    "    def assignPoints(self, X):\n",
    "        dist = distance_matrix(X, self.centroids)\n",
    "        self.clusters = defaultdict(list)\n",
    "        for i,item in enumerate(dist):\n",
    "            self.clusters[np.argmin(item)].append(X[i,:])\n",
    "\n",
    "    def __init__(self, X, k):\n",
    "        self.centroids = X[np.random.choice(X.shape[0], k, replace=False), :]\n",
    "        self.clusters = defaultdict(list)\n",
    "        self.assignPoints(X)\n",
    "        self.run(X)\n",
    "        print(self.centroids, self.clusters)\n",
    "\n",
    "    def run(self, X):\n",
    "        for i in range(5):\n",
    "            self.recomputeCetroid()\n",
    "            self.assignPoints(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means in PySpark!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "<img src=\"https://cdn-images-1.medium.com/max/902/1*Pa7PO1v7bANI7C-eHMS_PQ.png\"/> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you have the K-means algorithm written from scratch in PySpark. The next chunk has two useful functions needed for implementing the K-Means algorithm in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_vector(line):\n",
    "    return np.array([float(x) for x in line.split(' ')])\n",
    "\n",
    "\n",
    "def find_closest_point(p, centers):\n",
    "    bestIndex = 0\n",
    "    closest = float(\"+inf\")\n",
    "    for i in range(len(centers)):\n",
    "        tempDist = np.sum((p - centers[i]) ** 2)\n",
    "        if tempDist < closest:\n",
    "            closest = tempDist\n",
    "            bestIndex = i\n",
    "    return bestIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final centers: [array([15.5, 16.5, 17.5]), array([100., 101., 102.])]\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"PythonKMeans\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "lines_of_textfile = spark.read.text('points_in_the_space.txt').rdd.map(lambda r: r[0])\n",
    "data = lines_of_textfile.map(string_to_vector).cache()\n",
    "num_of_centers = int(2) #number of centers\n",
    "epsilon = float(0.001) #value of convergence\n",
    "\n",
    "kPoints = data.takeSample(False, num_of_centers, 1)\n",
    "tempDist = 1.0\n",
    "\n",
    "while tempDist > epsilon:\n",
    "    closest_points = data.map(lambda p: (find_closest_point(p, kPoints), (p, 1)))\\\n",
    "                         .reduceByKey(lambda p1_c1, p2_c2: (p1_c1[0] + p2_c2[0], p1_c1[1] + p2_c2[1]))\\\n",
    "                         .map(lambda st: (st[0], st[1][0] / st[1][1])).collect()\n",
    "\n",
    "    tempDist = sum(np.sum((kPoints[iK] - p) ** 2) for (iK, p) in closest_points)\n",
    "\n",
    "    for (iK, p) in closest_points:\n",
    "        kPoints[iK] = p\n",
    "\n",
    "print(\"Final Centroids: \" + str(kPoints))\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the duplicates!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "<img src=\"http://www.cse.chalmers.se/edu/course/TDA352/assets/images/avatar.jpg\"/> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task we've decided to use the methods for circular hashing. <br>\n",
    "In particular what we're supposed to do is implement an hash function to represent a string with a number inependently of the position of the characters inside the string. <br>\n",
    "In this case, the important thing is that the function we apply respect the commutative property, because in this case we can run the hash function on an arbitrary permutation of the input string and getting the same output. <br>\n",
    "Another important thing is that the function is nonlinear because: <br>\n",
    "\n",
    "citation: *\"first, it usually does not matter that much in practice, but if you really care, you should know that it is a research subject by itself. There are thousand of papers about that. You can still get a PhD today by studying & designing hashing algorithms. second hash function might be slightly better, because it probably should separate the string \"ab\" from the string \"ba\". On the other hand, it is probably less quick than the first hash function. It may, or may not, be relevant for your application.\"*\n",
    "<br>\n",
    "    \n",
    "\n",
    "So a closed form for getting the number hash number associated to a string is: <br><br>\n",
    "\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "$H(s) ~ = ~ (\\bigoplus_{c\\in s}{ord(c)*A << B}) ~ \\% ~ 2^{C}$ \n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "Where:\n",
    "\n",
    "$\\bigoplus$ is the [XOR](https://en.wikipedia.org/wiki/Exclusive_or) bitwise function over all possible characters inside the string; commutative and nonlinear, it gives us awesome properties. <br>\n",
    "\n",
    "$ord(c)$ is the mapping between characters and [ascii code](https://en.wikipedia.org/wiki/ASCII) <br>\n",
    "\n",
    "A is a very big prime number: in our case A = $7*37^{17}-27$ it helps to spread out the range of values and also reduce the false positives.<br>\n",
    "\n",
    "B is the bitwise function [\"left shift\"](https://msdn.microsoft.com/it-it/library/336xbhcz.aspx), actually it's an integer multiplier. In our case B = $2$. \n",
    "\n",
    "C is any power of 2 big enough to contain all possible values for the numbers associated to the string but not a [large number](https://en.wikipedia.org/wiki/Large_numbers), because it will compress the image set and it will be useful in the implementation with hash tables for avoid to wast memory space. In our case C = $2^{128}$\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashnoposition(s):\n",
    "    \"\"\"\n",
    "    s[0]*31^(n-1) + s[1]*31^(n-2) + ... + s[n-1]\n",
    "    This hash function maps a string into a number.\n",
    "    It doesen't matter the position of the characters in the string\n",
    "    \"\"\"\n",
    "    somme = 0\n",
    "    for c in s:\n",
    "        somme = (somme  ^ (ord(c)*(7*37**17-27) << 2))\n",
    "    somme % (2**128)\n",
    "    return somme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A brief demonstration of the working criteria is the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(hashnoposition('blue da ba dee da ba daa') == hashnoposition('da blbaue  dee da ba daa'))\n",
    "print(hashnoposition('+TjZfQ%CKtz/iOG9$U1P') == hashnoposition('th9Sb;zk$$Bn.Y&VdpZyr'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a function to get the range of values in which our hash function maps the strings in the __passwords2.txt__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "383353366233471895041246117920 995562401174542057826049359904\n"
     ]
    }
   ],
   "source": [
    "def find_the_range(hashfoo):\n",
    "    lis = []\n",
    "    with open('passwords2.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            lis.append(hashfoo(line))\n",
    "    f.close()\n",
    "    return (min(lis), max(lis))\n",
    "\n",
    "minim, maxim = find_the_range(hashnoposition)\n",
    "print(minim, maxim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next chunk is used to provide the number of false positives and the duplicates that are in the file __passwords2.txt__. <br>\n",
    "\n",
    "The strategy is to use a dictionary to store the informations you have already seen, the key is the hash code of an arbitrary string and the value is the string itself. <br> When when you try to insert a new element in this dictionary increment a counter of duplicates if the element is already in; if the string linked to that hash code is different to the actual one, the counter of collisions is incremented by 1. <br> For the collision finder we've sorted the lines so you don't get false positives due to the fact that the first hash function is [rotational invariant](https://en.wikipedia.org/wiki/Rotational_invariance) and doesn't care about the position. <br> The parameter __hashfoo__ is a pointer to an arbitrary hash function, it's very useful if you don't want to have an [anti pattern](https://en.wikipedia.org/wiki/Anti-pattern) with a useless duplicate code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collisions = 5773, Duplicates = 10005773\n"
     ]
    }
   ],
   "source": [
    "def find_collision(hashfoo):\n",
    "    founds = dict()\n",
    "    coll = 0 #number of collisions\n",
    "    dup = 0 #number of duplicates\n",
    "    with open('passwords2.txt', 'r') as f: \n",
    "        for i, line in enumerate(f):\n",
    "            hashval = hashfoo(line)\n",
    "            if hashval in founds.keys():\n",
    "                dup += 1 #i have found a duplicate\n",
    "                if sorted(founds[hashval]) != sorted(line):\n",
    "                    coll +=1\n",
    "            else:\n",
    "                founds[hashval] = line\n",
    "    f.close()\n",
    "    return (coll,dup)\n",
    "\n",
    "collisions, duplicates = find_collision(hashnoposition)\n",
    "print(\"Collisions =\", collisions, \"Duplicates =\", duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order matters!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've to define another hash function, but this time we've to keep in mind that two strings are the same if all the characters are equals elementwise. <br>\n",
    "We've read something on the internet and for doing this we've decided to use a double XOR operator, a left shift and a product between the hash code itself with a big prime number while you compute it. <br>\n",
    "Now we have a pretty nonlinear, noncommutative and nonassociative function. In maths the class of this function is called [Nonlinear Invariant Galois Fiels](http://www.singacom.uva.es/~iremarquez/CACTC2016/CACTC16-5.pdf). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashposition(s):\n",
    "    \"\"\"\n",
    "    This function define the hash code of a string.\n",
    "    In this hash function the position has importance\n",
    "    \"\"\"\n",
    "    x = ord(s[0]) << 7\n",
    "    for c in s[1:]:\n",
    "        x = (1000003 * x) ^ ~ ord(c)  & ~ x  |  37 >> ord(c)\n",
    "    x = x ^ len(s)\n",
    "    return abs(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you have a brief demonstration that the previous hash function works well even if the characters of the compared string (S1) are a permutation of the characters of the comparing string (S)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(hashposition('blue da ba dee da ba daa') == hashposition('da blbaue  dee da ba daa'))\n",
    "print(hashposition('+TjZfQ%CKtz/iOG9$U1P') == hashposition('th9Sb;zk$$Bn.Y&VdpZyr'))\n",
    "print(hashposition('+TjZfQ%CKtfantaniz/iOG9$U1P') == hashposition('+TjZfQ%CKtfantaniz/iOG9$U1P'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to run the function for finding the duplicates and the collisions over the same dataset but now we use the __hashposition__ function so we expect that the duplicates are quietly different from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collisions = 0 Duplicates = 5000000\n"
     ]
    }
   ],
   "source": [
    "collisions, duplicates = find_collision(hashposition)\n",
    "print(\"Collisions =\", collisions, \"Duplicates =\", duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: Duplicate Finder in Spark!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have an implementation in Spark of what we did before. <br> The important thing is to check if the Spark results are the same of what we did without it. <br> First of all we have to define the Spark session and read the text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"PythonDuplicateFinder\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "txt = spark.read.text('passwords3.txt').rdd.map(lambda r: r[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having the entire text file in a variable named __txt__ we have only to define the MapReduce pipeline. <br>\n",
    "First we emit the string and its hash in order to have a good way to recognize different pairs __<S, H(S)>__. <br>\n",
    "Second we count the same pairs __<S, H(S)>__ emitted before, then we filter out the pairs that appear only once. <br>\n",
    "Finally we take all the strings that mave matched as duplicates and put them into an RDD. The length of that RDD will be the number of duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates in which the order matters = 5000000\n"
     ]
    }
   ],
   "source": [
    "duplicate_strings_with_position = txt.map(lambda x: ((hashposition(x),x ), 1))\\\n",
    "                                   .reduceByKey(lambda x,y: x+y)\\\n",
    "                                   .filter(lambda x: x[1] != 1)\\\n",
    "                                   .map(lambda x: (x[0][1]))\n",
    "\n",
    "print(\"Number of duplicates in which the order matters =\", len(duplicate_strings_with_position.take(int(1e100))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected the number of duplicates is the same with and without Spark.\n",
    "\n",
    "Here we have the same code but now the hash function is the one that does not take into account the position of the characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates in which the order don't matters = 10005773\n"
     ]
    }
   ],
   "source": [
    "duplicate_strings_without_position = txt.map(lambda x: ((hashnoposition(x),x ), 1))\\\n",
    "                                      .reduceByKey(lambda x,y: x+y)\\\n",
    "                                      .filter(lambda x: x[1] != 1)\\\n",
    "                                      .map(lambda x: (x[0][1]))\n",
    "\n",
    "\n",
    "print(\"Number of duplicates in which the order don't matters =\", len(duplicate_strings_without_position.take(int(1e100))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected the number of duplicates is the same with and without Spark. Of course you don't see exactly 10M of duplicates because of the collisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
